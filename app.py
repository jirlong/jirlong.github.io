import streamlit as st
import jieba
import pandas as pd
from collections import Counter
import re
import csv
from io import StringIO

# è¨­å®šé é¢é…ç½®
st.set_page_config(
    page_title="ä¸­æ–‡åˆ†è©å·¥å…·",
    layout="wide"
)

def load_stopwords(file_path):
    """è¼‰å…¥åœç”¨è©"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    except FileNotFoundError:
        st.warning(f"æ‰¾ä¸åˆ°åœç”¨è©æª”æ¡ˆï¼š{file_path}")
        return set()

def load_user_dict(file_path):
    """è¼‰å…¥è‡ªå®šç¾©è©å…¸"""
    try:
        jieba.load_userdict(file_path)
    except FileNotFoundError:
        st.warning(f"æ‰¾ä¸åˆ°è‡ªå®šç¾©è©å…¸ï¼š{file_path}")

# è¼‰å…¥åœç”¨è©å’Œè‡ªå®šç¾©è©å…¸
STOPWORDS = load_stopwords('data/stopwords_zh-tw.txt')
load_user_dict('data/userdict.txt')

def remove_punctuation_and_english(text):
    """ç§»é™¤æ¨™é»ç¬¦è™Ÿã€è‹±æ–‡å­—æ¯å’Œæ•¸å­—"""
    # ç§»é™¤ä¸­æ–‡æ¨™é»ç¬¦è™Ÿ
    text = re.sub(r'[\u3000-\u303F\uFF00-\uFFEF]', '', text)
    # ç§»é™¤è‹±æ–‡æ¨™é»ç¬¦è™Ÿ
    text = re.sub(r'[^\u4e00-\u9fff\s]', '', text)
    return text

def process_text(text, remove_stopwords=True):
    """è™•ç†æ–‡æœ¬ï¼Œé€²è¡Œåˆ†è©ä¸¦é¸æ“‡æ€§åœ°ç§»é™¤åœç”¨è©"""
    # æ¸…ç†æ–‡æœ¬
    text = remove_punctuation_and_english(text)
    
    # ä½¿ç”¨çµå·´åˆ†è©
    tokens = list(jieba.cut(text))
    
    # ç§»é™¤ç©ºå­—ä¸²
    tokens = [token for token in tokens if token.strip()]
    
    if remove_stopwords:
        tokens = [token for token in tokens if token not in STOPWORDS]
    
    return tokens

def process_paragraphs(text, remove_stopwords=True):
    """è™•ç†å¤šå€‹æ®µè½çš„æ–‡æœ¬"""
    # ä¾æ“šæ›è¡Œç¬¦åˆ†å‰²æ®µè½
    paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
    
    # è™•ç†æ¯å€‹æ®µè½
    processed_paragraphs = []
    all_tokens = []
    
    for paragraph in paragraphs:
        tokens = process_text(paragraph, remove_stopwords)
        if tokens:  # åªåŠ å…¥éç©ºçš„çµæœ
            processed_paragraphs.append(tokens)
            all_tokens.extend(tokens)
    
    return processed_paragraphs, all_tokens

def calculate_statistics(tokens):
    """è¨ˆç®—æ–‡æœ¬çµ±è¨ˆä¿¡æ¯"""
    # è¨ˆç®—è©é »
    word_freq = Counter(tokens)
    
    # è½‰æ›æˆ DataFrame ä»¥ä¾¿é¡¯ç¤º
    df = pd.DataFrame(word_freq.most_common(), columns=['è©èª', 'å‡ºç¾æ¬¡æ•¸'])
    
    return {
        'total_tokens': len(tokens),
        'unique_tokens': len(set(tokens)),
        'word_freq': df
    }

def create_download_csv(processed_paragraphs):
    """å»ºç«‹ä¸‹è¼‰ç”¨çš„ CSV å…§å®¹"""
    output = StringIO()
    writer = csv.writer(output)
    # æ¯å€‹æ®µè½å¯«å…¥ä¸€è¡Œï¼Œè©èªç”¨é€—è™Ÿåˆ†éš”
    for paragraph_tokens in processed_paragraphs:
        writer.writerow(paragraph_tokens)
    return output.getvalue()

def main():
    # æ¨™é¡Œ
    st.title("ä¸­æ–‡åˆ†è©å·¥å…· ğŸ”¤")
    
    # å´é‚Šæ¬„è¨­ç½®
    with st.sidebar:
        st.header("è¨­å®š")
        remove_stopwords = st.checkbox("ç§»é™¤åœç”¨è©", value=True)
        show_statistics = st.checkbox("é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š", value=True)
        
        st.header("èªªæ˜")
        st.markdown("""
        é€™æ˜¯ä¸€å€‹åŸºæ–¼ jieba çš„ä¸­æ–‡åˆ†è©å·¥å…·ã€‚
        
        åŠŸèƒ½ï¼š
        - æ”¯æ´å¤šæ®µè½ä¸­æ–‡åˆ†è©
        - è‡ªå‹•è¼‰å…¥è‡ªå®šç¾©è©å…¸
        - è‡ªå‹•è¼‰å…¥åœç”¨è©åˆ—è¡¨
        - è‡ªå‹•ç§»é™¤æ¨™é»ç¬¦è™Ÿã€è‹±æ–‡å’Œæ•¸å­—
        - æä¾›æ–‡æœ¬çµ±è¨ˆåˆ†æ
        - è©é »çµ±è¨ˆèˆ‡è¦–è¦ºåŒ–
        - åˆ†æ®µè½é¡¯ç¤ºåˆ†è©çµæœ
        - CSV åŒ¯å‡ºæ”¯æ´å¤šæ®µè½æ ¼å¼
        """)
    
    # ä¸»è¦å…§å®¹å€
    col1, col2 = st.columns([1, 1])
    
    with col1:
        # æ–‡æœ¬è¼¸å…¥å€
        text_input = st.text_area(
            "è«‹è¼¸å…¥è¦åˆ†è©çš„æ–‡å­—ï¼ˆå¯è¼¸å…¥å¤šå€‹æ®µè½ï¼‰ï¼š",
            height=200,
            placeholder="åœ¨æ­¤è¼¸å…¥ä¸­æ–‡æ–‡å­—...\nå¯ä»¥æ›è¡Œè¼¸å…¥å¤šå€‹æ®µè½..."
        )
        
        if text_input:
            # é€²è¡Œåˆ†è©
            processed_paragraphs, all_tokens = process_paragraphs(text_input, remove_stopwords)
            
            # é¡¯ç¤ºåˆ†è©çµæœ
            st.header("åˆ†è©çµæœ")
            for i, paragraph_tokens in enumerate(processed_paragraphs, 1):
                st.subheader(f"ç¬¬ {i} æ®µ")
                st.write(" | ".join(paragraph_tokens))
            
            # æä¾›ä¸‹è¼‰æŒ‰éˆ•
            csv_content = create_download_csv(processed_paragraphs)
            st.download_button(
                label="ä¸‹è¼‰åˆ†è©çµæœ",
                data=csv_content.encode('utf-8'),
                file_name='tokens.csv',
                mime='text/csv'
            )
    
    with col2:
        if text_input and show_statistics:
            # è¨ˆç®—ä¸¦é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
            stats = calculate_statistics(all_tokens)
            
            st.header("çµ±è¨ˆè³‡è¨Š")
            
            # åŸºæœ¬çµ±è¨ˆ
            col_stats1, col_stats2 = st.columns(2)
            with col_stats1:
                st.metric("ç¸½è©æ•¸", stats['total_tokens'])
            with col_stats2:
                st.metric("ä¸é‡è¤‡è©æ•¸", stats['unique_tokens'])
            
            # è©é »çµ±è¨ˆè¡¨
            st.subheader("è©é »çµ±è¨ˆ")
            st.dataframe(
                stats['word_freq'].head(10),
                use_container_width=True
            )
            
            # è©é »è¦–è¦ºåŒ–
            if len(stats['word_freq']) > 0:
                st.subheader("è©é »åˆ†ä½ˆ")
                chart_data = stats['word_freq'].head(10)
                st.bar_chart(
                    chart_data.set_index('è©èª')['å‡ºç¾æ¬¡æ•¸'],
                    use_container_width=True
                )

if __name__ == "__main__":
    main()